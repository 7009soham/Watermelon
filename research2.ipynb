{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8494cfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.249691\n",
      "Epoch 100, Loss: 0.008891\n",
      "Epoch 200, Loss: 0.007605\n",
      "Epoch 300, Loss: 0.004961\n",
      "Epoch 400, Loss: 0.005419\n",
      "Epoch 500, Loss: 0.003808\n",
      "Epoch 600, Loss: 0.003697\n",
      "Epoch 700, Loss: 0.003618\n",
      "Epoch 800, Loss: 0.003554\n",
      "Epoch 900, Loss: 0.003495\n",
      "\n",
      "--- Training Complete ---\n",
      "Final Predicted Output (rounded):\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "\n",
      "Actual Labels:\n",
      "[0 1 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0\n",
      " 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 1 0\n",
      " 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0\n",
      " 0 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1\n",
      " 0 1 0 1 0 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1\n",
      " 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0\n",
      " 1 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Activation Functions ---\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return z * (1 - z)\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "# Load CSVs\n",
    "natural = pd.read_csv(\"natural_watermelon_data.csv\")\n",
    "adulterated = pd.read_csv(\"adulterated_watermelon_data.csv\")\n",
    "\n",
    "# Assign labels: natural = 1, adulterated = 0\n",
    "natural[\"label\"] = 1\n",
    "adulterated[\"label\"] = 0\n",
    "\n",
    "# Combine into one dataset\n",
    "data = pd.concat([natural, adulterated], axis=0).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Split into features (X) and labels (Y)\n",
    "X = data.drop(\"label\", axis=1).values\n",
    "Y = data[\"label\"].values.reshape(-1, 1)   # column vector\n",
    "\n",
    "# Normalize features (important for training stability)\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# --- 2. Define Network Architecture ---\n",
    "input_neurons = X.shape[1]\n",
    "hidden_neurons = 64\n",
    "output_neurons = 1\n",
    "\n",
    "# --- 3. Initialize Weights & Biases ---\n",
    "wh = np.random.randn(input_neurons, hidden_neurons) * 0.01\n",
    "bh = np.zeros((1, hidden_neurons))\n",
    "wout = np.random.randn(hidden_neurons, output_neurons) * 0.01\n",
    "bout = np.zeros((1, output_neurons))\n",
    "\n",
    "# --- 4. Train the Network ---\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Forward Propagation\n",
    "    hidden_layer_input = np.dot(X, wh) + bh\n",
    "    hidden_layer_activations = relu(hidden_layer_input)\n",
    "\n",
    "    output_layer_input = np.dot(hidden_layer_activations, wout) + bout\n",
    "    predicted_output = sigmoid(output_layer_input)\n",
    "\n",
    "    # Backward Propagation\n",
    "    error = Y - predicted_output\n",
    "    d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
    "\n",
    "    error_hidden_layer = d_predicted_output.dot(wout.T)\n",
    "    d_hidden_layer = error_hidden_layer * relu_derivative(hidden_layer_input)\n",
    "\n",
    "    # Update weights\n",
    "    wout += hidden_layer_activations.T.dot(d_predicted_output) * learning_rate\n",
    "    bout += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate\n",
    "    wh += X.T.dot(d_hidden_layer) * learning_rate\n",
    "    bh += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    # Print progress\n",
    "    if i % 100 == 0:\n",
    "        loss = np.mean(np.square(Y - predicted_output))\n",
    "        print(f\"Epoch {i}, Loss: {loss:.6f}\")\n",
    "\n",
    "# --- 5. Display Results ---\n",
    "print(\"\\n--- Training Complete ---\")\n",
    "print(\"Final Predicted Output (rounded):\")\n",
    "print(np.round(predicted_output))\n",
    "print(\"\\nActual Labels:\")\n",
    "print(Y.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cea35ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.249911\n",
      "Epoch 100, Loss: 0.012078\n",
      "Epoch 200, Loss: 0.009905\n",
      "Epoch 300, Loss: 0.011730\n",
      "Epoch 400, Loss: 0.005609\n",
      "Epoch 500, Loss: 0.004708\n",
      "Epoch 600, Loss: 0.004558\n",
      "Epoch 700, Loss: 0.004451\n",
      "Epoch 800, Loss: 0.004362\n",
      "Epoch 900, Loss: 0.004280\n",
      "\n",
      "--- Training Complete ---\n",
      "âœ… Test Accuracy: 100.00%\n",
      "\n",
      "Predictions on test set:\n",
      "[0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1.\n",
      " 0. 1.]\n",
      "\n",
      "Actual labels:\n",
      "[0 0 1 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 0\n",
      " 1 0 0 1 0 1 0 1 1 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Activation Functions ---\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return z * (1 - z)\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "natural = pd.read_csv(\"natural_watermelon_data.csv\")\n",
    "adulterated = pd.read_csv(\"adulterated_watermelon_data.csv\")\n",
    "\n",
    "natural[\"label\"] = 1\n",
    "adulterated[\"label\"] = 0\n",
    "\n",
    "data = pd.concat([natural, adulterated], axis=0).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "X = data.drop(\"label\", axis=1).values\n",
    "Y = data[\"label\"].values.reshape(-1, 1)\n",
    "\n",
    "# Normalize features\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# --- Manual Train-Test Split (80% train / 20% test) ---\n",
    "split_idx = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "Y_train, Y_test = Y[:split_idx], Y[split_idx:]\n",
    "\n",
    "# --- 2. Define Network Architecture ---\n",
    "input_neurons = X_train.shape[1]\n",
    "hidden_neurons = 64\n",
    "output_neurons = 1\n",
    "\n",
    "# --- 3. Initialize Weights & Biases ---\n",
    "wh = np.random.randn(input_neurons, hidden_neurons) * 0.01\n",
    "bh = np.zeros((1, hidden_neurons))\n",
    "wout = np.random.randn(hidden_neurons, output_neurons) * 0.01\n",
    "bout = np.zeros((1, output_neurons))\n",
    "\n",
    "# --- 4. Train the Network ---\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Forward pass\n",
    "    hidden_input = np.dot(X_train, wh) + bh\n",
    "    hidden_activations = relu(hidden_input)\n",
    "\n",
    "    output_input = np.dot(hidden_activations, wout) + bout\n",
    "    predicted_output = sigmoid(output_input)\n",
    "\n",
    "    # Backward pass\n",
    "    error = Y_train - predicted_output\n",
    "    d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
    "\n",
    "    error_hidden = d_predicted_output.dot(wout.T)\n",
    "    d_hidden = error_hidden * relu_derivative(hidden_input)\n",
    "\n",
    "    # Update weights\n",
    "    wout += hidden_activations.T.dot(d_predicted_output) * learning_rate\n",
    "    bout += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate\n",
    "    wh += X_train.T.dot(d_hidden) * learning_rate\n",
    "    bh += np.sum(d_hidden, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        loss = np.mean(np.square(Y_train - predicted_output))\n",
    "        print(f\"Epoch {i}, Loss: {loss:.6f}\")\n",
    "\n",
    "# --- 5. Evaluate on Test Data ---\n",
    "hidden_test = relu(np.dot(X_test, wh) + bh)\n",
    "test_pred = sigmoid(np.dot(hidden_test, wout) + bout)\n",
    "test_pred_rounded = np.round(test_pred)\n",
    "\n",
    "accuracy = np.mean(test_pred_rounded == Y_test)\n",
    "print(\"\\n--- Training Complete ---\")\n",
    "print(f\"âœ… Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nPredictions on test set:\")\n",
    "print(test_pred_rounded.flatten())\n",
    "print(\"\\nActual labels:\")\n",
    "print(Y_test.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8c82bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "TP: 24, FP: 0, FN: 0, TN: 26\n"
     ]
    }
   ],
   "source": [
    "# --- Confusion Matrix ---\n",
    "tp = np.sum((test_pred_rounded == 1) & (Y_test == 1))\n",
    "tn = np.sum((test_pred_rounded == 0) & (Y_test == 0))\n",
    "fp = np.sum((test_pred_rounded == 1) & (Y_test == 0))\n",
    "fn = np.sum((test_pred_rounded == 0) & (Y_test == 1))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(f\"TP: {tp}, FP: {fp}, FN: {fn}, TN: {tn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5adbe08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Complete ---\n",
      "âœ… Test Accuracy: 100.00%\n",
      "\n",
      "Predictions on test set:\n",
      "[0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1.\n",
      " 0. 1.]\n",
      "\n",
      "Actual labels:\n",
      "[0 0 1 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 0\n",
      " 1 0 0 1 0 1 0 1 1 1 1 0 1]\n",
      "\n",
      "Confusion Matrix:\n",
      "TP: 24, FP: 0, FN: 0, TN: 26\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Evaluate on Test Data ---\n",
    "hidden_test = relu(np.dot(X_test, wh) + bh)\n",
    "test_pred = sigmoid(np.dot(hidden_test, wout) + bout)\n",
    "test_pred_rounded = np.round(test_pred)\n",
    "\n",
    "accuracy = np.mean(test_pred_rounded == Y_test)\n",
    "print(\"\\n--- Training Complete ---\")\n",
    "print(f\"âœ… Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nPredictions on test set:\")\n",
    "print(test_pred_rounded.flatten())\n",
    "print(\"\\nActual labels:\")\n",
    "print(Y_test.flatten())\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "tp = np.sum((test_pred_rounded == 1) & (Y_test == 1))\n",
    "tn = np.sum((test_pred_rounded == 0) & (Y_test == 0))\n",
    "fp = np.sum((test_pred_rounded == 1) & (Y_test == 0))\n",
    "fn = np.sum((test_pred_rounded == 0) & (Y_test == 1))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(f\"TP: {tp}, FP: {fp}, FN: {fn}, TN: {tn}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
